# Natural Language Processing - DesafÃ­os UBA

Bienvenido al repositorio de **Natural Language Processing** desarrollado para los **desafÃ­os de la UBA**.  
AquÃ­ se exploran conceptos fundamentales del procesamiento de lenguaje natural, desde representaciones vectoriales bÃ¡sicas hasta arquitecturas encoder-decoder aplicadas a sistemas de preguntas y respuestas.

## ğŸ“‚ Estructura del repositorio

- `ejercicios/`
  - `desafio-1/`: VectorizaciÃ³n, Similitud y ClasificaciÃ³n de Texto
  - `desafio-2/`: Word Embeddings usando Word2Vec
  - `desafio-3/`: Modelo de Lenguaje con TokenizaciÃ³n por Caracteres
  - `desafio-4/`: Bot de Preguntas y Respuestas (QA) con Encoder/Decoder

Cada carpeta contiene un notebook `desafio.ipynb` o `desafio_4.ipynb` con el desarrollo completo.

## ğŸ“– Resumen de cada desafÃ­o

### ğŸ§  DesafÃ­o 1 - VectorizaciÃ³n, Similitud y ClasificaciÃ³n de Texto

ğŸ”— [Ver notebook](https://github.com/DevJulianSalas/natural-language-processing/blob/main/ejercicios/desafio-1/desafio.ipynb)

- **Objetivo**: Transformar textos en representaciones numÃ©ricas para comparar y clasificar documentos.
- **Contenido**:
  - VectorizaciÃ³n usando `CountVectorizer` y `TfidfVectorizer`.
  - CÃ¡lculo de similitud de documentos mediante `cosine_similarity`.
  - ClasificaciÃ³n con **Naive Bayes Multinomial** y **Complementario**.
  - EvaluaciÃ³n con **F1-Score**.
- **Conceptos clave**: RepresentaciÃ³n de texto, mÃ©tricas de similitud, clasificaciÃ³n supervisada.

### ğŸ“š DesafÃ­o 2 - Word Embeddings usando Word2Vec

ğŸ”— [Ver notebook](https://github.com/DevJulianSalas/natural-language-processing/blob/main/ejercicios/desafio-2/desafio.ipynb)

- **Objetivo**: Construir representaciones semÃ¡nticas de palabras en espacios vectoriales.
- **Contenido**:
  - Preprocesamiento de texto usando el corpus **Gutenberg**.
  - Entrenamiento de **Word2Vec** (arquitectura **Skip-gram**).
  - AnÃ¡lisis de relaciones semÃ¡nticas a partir de embeddings.
- **Conceptos clave**: SemÃ¡ntica distribuida, representaciones vectoriales, skip-gram.

### âœï¸ DesafÃ­o 3 - Modelo de Lenguaje con TokenizaciÃ³n por Caracteres

ğŸ”— [Ver notebook](https://github.com/DevJulianSalas/natural-language-processing/blob/main/ejercicios/desafio-3/desafio.ipynb)

- **Objetivo**: Entrenar un modelo de lenguaje bÃ¡sico carÃ¡cter por carÃ¡cter.
- **Contenido**:
  - TokenizaciÃ³n a nivel de carÃ¡cter.
  - PreparaciÃ³n de datasets de secuencias.
  - Entrenamiento de una red neuronal (Embedding + LSTM).
  - PredicciÃ³n de caracteres futuros.
- **Conceptos clave**: Modelado secuencial, dependencias locales, generaciÃ³n de texto.

### ğŸ¤– DesafÃ­o 4 - Bot de Preguntas y Respuestas (QA) con Encoder/Decoder

ğŸ”— [Ver notebook](https://github.com/DevJulianSalas/natural-language-processing/blob/main/ejercicios/desafio-4/desafio_4.ipynb)

- **Objetivo**: Construir un sistema de preguntas y respuestas utilizando una arquitectura seq2seq.
- **Contenido**:
  - Preprocesamiento de preguntas y respuestas.
  - ImplementaciÃ³n de un modelo **Encoder-Decoder**.
  - Uso de **teacher forcing** para mejorar el entrenamiento.
  - GeneraciÃ³n automÃ¡tica de respuestas.
- **Conceptos clave**: CompresiÃ³n de contexto, generaciÃ³n secuencial, respuestas a preguntas.

## ğŸš€ Â¿CÃ³mo ejecutar los desafÃ­os?

### 1. Clonar el repositorio

```bash
git clone https://github.com/DevJulianSalas/natural-language-processing.git
cd natural-language-processing
```

### 2. Clonar el repositorio
```bash
pip install -r requirements.txt
```

### 3. Ejecutar el notebook
```bash
jupyter notebook
```


### 4. Abrir el notebook deseado
Desde el navegador accede a la carpeta ejercicios/ y selecciona el desafÃ­o que quieras explorar.

âœ… Opcional: Crear un entorno virtual para aislar dependencias:

```bash
python -m venv venv
source venv/bin/activate  
venv\Scripts\activate.bat  
pip install -r requirements.txt
```

### ğŸ¯ Conclusiones Generales
- RepresentaciÃ³n de texto: Aprendimos a transformar texto en vectores numÃ©ricos simples (BOW, TF-IDF) y avanzados (embeddings).

- Similitud y clasificaciÃ³n: Medimos similitud semÃ¡ntica entre textos y aplicamos tÃ©cnicas clÃ¡sicas de clasificaciÃ³n.

- Modelado secuencial: Introdujimos dependencias temporales mediante tokenizaciÃ³n por caracteres y redes LSTM.

- Modelos generativos: Implementamos un pipeline completo de entrada/salida usando Encoder/Decoder, abriendo la puerta a tareas mÃ¡s complejas como chatbots, traducciÃ³n automÃ¡tica o resumen de texto.



ğŸ§  ReflexiÃ³n final: La comprensiÃ³n profunda de cÃ³mo representamos, procesamos y generamos secuencias es esencial para resolver problemas reales de NLP y preparar el terreno hacia arquitecturas aÃºn mÃ¡s potentes como Transformers.