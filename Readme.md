# Natural Language Processing - Desaf√≠os UBA

Bienvenido al repositorio de **Natural Language Processing** desarrollado para los **desaf√≠os de la UBA**.  
Aqu√≠ se exploran conceptos fundamentales del procesamiento de lenguaje natural, desde representaciones vectoriales b√°sicas hasta arquitecturas encoder-decoder aplicadas a sistemas de preguntas y respuestas.

## üìÇ Estructura del repositorio

- `ejercicios/`
  - `desafio-1/`: Vectorizaci√≥n, Similitud y Clasificaci√≥n de Texto
  - `desafio-2/`: Word Embeddings usando Word2Vec
  - `desafio-3/`: Modelo de Lenguaje con Tokenizaci√≥n por Caracteres
  - `desafio-4/`: Bot de Preguntas y Respuestas (QA) con Encoder/Decoder

Cada carpeta contiene un notebook `desafio.ipynb` o `desafio_4.ipynb` con el desarrollo completo.

## üìñ Resumen de cada desaf√≠o

### üß† Desaf√≠o 1 - Vectorizaci√≥n, Similitud y Clasificaci√≥n de Texto

üîó [Ver notebook](https://github.com/DevJulianSalas/natural-language-processing/blob/main/ejercicios/desafio-1/desafio.ipynb)

- **Objetivo**: Transformar textos en representaciones num√©ricas para comparar y clasificar documentos.
- **Contenido**:
  - Vectorizaci√≥n usando `CountVectorizer` y `TfidfVectorizer`.
  - C√°lculo de similitud de documentos mediante `cosine_similarity`.
  - Clasificaci√≥n con **Naive Bayes Multinomial** y **Complementario**.
  - Evaluaci√≥n con **F1-Score**.
- **Conceptos clave**: Representaci√≥n de texto, m√©tricas de similitud, clasificaci√≥n supervisada.

### üìö Desaf√≠o 2 - Word Embeddings usando Word2Vec

üîó [Ver notebook](https://github.com/DevJulianSalas/natural-language-processing/blob/main/ejercicios/desafio-2/desafio.ipynb)

- **Objetivo**: Construir representaciones sem√°nticas de palabras en espacios vectoriales.
- **Contenido**:
  - Preprocesamiento de texto usando el corpus **Gutenberg**.
  - Entrenamiento de **Word2Vec** (arquitectura **Skip-gram**).
  - An√°lisis de relaciones sem√°nticas a partir de embeddings.
- **Conceptos clave**: Sem√°ntica distribuida, representaciones vectoriales, skip-gram.

### ‚úçÔ∏è Desaf√≠o 3 - Modelo de Lenguaje con Tokenizaci√≥n por Caracteres

üîó [Ver notebook](https://github.com/DevJulianSalas/natural-language-processing/blob/main/ejercicios/desafio-3/desafio.ipynb)

- **Objetivo**: Entrenar un modelo de lenguaje b√°sico car√°cter por car√°cter.
- **Contenido**:
  - Tokenizaci√≥n a nivel de car√°cter.
  - Preparaci√≥n de datasets de secuencias.
  - Entrenamiento de una red neuronal (Embedding + LSTM).
  - Predicci√≥n de caracteres futuros.
- **Conceptos clave**: Modelado secuencial, dependencias locales, generaci√≥n de texto.

### ü§ñ Desaf√≠o 4 - Bot de Preguntas y Respuestas (QA) con Encoder/Decoder

üîó [Ver notebook](https://github.com/DevJulianSalas/natural-language-processing/blob/main/ejercicios/desafio-4/desafio_4.ipynb)

- **Objetivo**: Construir un sistema de preguntas y respuestas utilizando una arquitectura seq2seq.
- **Contenido**:
  - Preprocesamiento de preguntas y respuestas.
  - Implementaci√≥n de un modelo **Encoder-Decoder**.
  - Uso de **teacher forcing** para mejorar el entrenamiento.
  - Generaci√≥n autom√°tica de respuestas.
- **Conceptos clave**: Compresi√≥n de contexto, generaci√≥n secuencial, traducci√≥n de secuencias.

## üöÄ ¬øC√≥mo ejecutar los desaf√≠os?

### 1. Clonar el repositorio

```bash
git clone https://github.com/DevJulianSalas/natural-language-processing.git
cd natural-language-processing
```

### 2. Clonar el repositorio
```bash
pip install -r requirements.txt
```

### 3. Ejecutar el notebook
```bash
jupyter notebook
```


### 4. Abrir el notebook deseado
Desde el navegador accede a la carpeta ejercicios/ y selecciona el desaf√≠o que quieras explorar.

‚úÖ Opcional: Crear un entorno virtual para aislar dependencias:

```bash
python -m venv venv
source venv/bin/activate  
venv\Scripts\activate.bat  
pip install -r requirements.txt
```

### üéØ Conclusiones Generales
- Representaci√≥n de texto: Aprendimos a transformar texto en vectores num√©ricos simples (BOW, TF-IDF) y avanzados (embeddings).

- Similitud y clasificaci√≥n: Medimos similitud sem√°ntica entre textos y aplicamos t√©cnicas cl√°sicas de clasificaci√≥n.

- Modelado secuencial: Introdujimos dependencias temporales mediante tokenizaci√≥n por caracteres y redes LSTM.

- Modelos generativos: Implementamos un pipeline completo de entrada/salida usando Encoder/Decoder, abriendo la puerta a tareas m√°s complejas como chatbots, traducci√≥n autom√°tica o resumen de texto.



üß† Reflexi√≥n final: La comprensi√≥n profunda de c√≥mo representamos, procesamos y generamos secuencias es esencial para resolver problemas reales de NLP y preparar el terreno hacia arquitecturas a√∫n m√°s potentes como Transformers.